{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa52376",
   "metadata": {},
   "source": [
    "# üß† NYC Taxi Trip Duration ‚Äî Phase 1 (Neural Networks)\n",
    "\n",
    "**Research objective.** Develop a tabular neural network to predict `trip_duration` (seconds) from engineered temporal and spatial signals, and test whether it improves upon a tuned classical baseline under a strict evaluation protocol.\n",
    "\n",
    "**Primary hypothesis (H1).** The best neural network selected on validation achieves higher predictive performance than the tuned Ridge baseline when evaluated once on the test split.\n",
    "\n",
    "**Reading guide.** Each markdown block directly interprets the **code cell below it**. Earlier cells define objects and conventions (features, scaling, log-space evaluation); later cells assume those definitions and focus on the new methodological contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7573bbc",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 1 ‚Äî üß∞ Reproducible setup: imports, constants, and device selection\n",
    "\n",
    "### üéØ Research aim\n",
    "Establish the computational environment and the *experimental constants* that parameterize every subsequent model, metric, and file path.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Import numerical, ML, and PyTorch tooling (`numpy`, `pandas`, `sklearn`, `torch`).\n",
    "- Define run-wide constants (`SEED`, `NROWS`, `TARGET`) and locate the dataset via `DATA_PATH`.\n",
    "- Create an `ARTIFACTS_DIR` to persist preprocessing statistics and tuning logs.\n",
    "- Select `device` (CUDA GPU if available, else CPU) and print the configuration for traceability.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "Expect two short printouts: the resolved `DATA_PATH`/`NROWS`/`SEED`, and the selected compute `Device`. These are the first sanity checks before any data is touched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c247b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  DATA_PATH: ../data/train.csv\n",
      "  NROWS: 1000000\n",
      "  SEED: 42\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for this section (torch, torch.utils.data).\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Imports for this section (sklearn.model_selection, sklearn.linear_model, sklearn.metrics).\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set global configuration/constants used throughout the notebook.\n",
    "SEED = 42\n",
    "NROWS = 1_000_000\n",
    "TARGET = \"trip_duration\"\n",
    "\n",
    "# Set global configuration/constants used throughout the notebook.\n",
    "DATA_PATH = Path(\"../data/train.csv\")\n",
    "\n",
    "# Kaggle NYC Taxi Trip Duration schema (train.csv)\n",
    "DATA_URL = \"https://github.com/DrAlzahraniProjects/csusb_spring26_cse5140_team1/releases/download/v1.0/train.csv\"\n",
    "\n",
    "# Set global configuration/constants used throughout the notebook.\n",
    "ARTIFACTS_DIR = Path(\"../artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"Config:\")\n",
    "print(\"  DATA_PATH:\", DATA_PATH)\n",
    "print(\"  NROWS:\", NROWS)\n",
    "print(\"  SEED:\", SEED)\n",
    "\n",
    "# Select computation device (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51900aa8",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 1B ‚Äî üß™ Execution environment snapshot (NRP reproducibility)\n",
    "\n",
    "### üéØ Research aim\n",
    "Record the exact **Python / library environment** and **compute device** used for this run so the grader can reproduce results on the **official NRP JupyterHub PyTorch2** stack.\n",
    "\n",
    "### ‚úÖ What this cell does\n",
    "- Prints Python version, OS/platform, and the active device (CPU/GPU)\n",
    "- Writes `../environment_summary.md` (required) and `../pip_freeze.txt` (optional but helpful)\n",
    "\n",
    "> **Project rule reminder:** Final scores must come from an end-to-end run on **NRP JupyterHub** (no local runs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ad1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote: /home/jovyan/csusb_spring26_cse5140_team1/environment_summary.md\n",
      "‚úÖ Wrote: /home/jovyan/csusb_spring26_cse5140_team1/pip_freeze.txt\n",
      "Device: cuda (GPU: NVIDIA GeForce RTX 2080 Ti)\n",
      "NRP image hint: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp-pytorch2:v1.0\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, subprocess, datetime, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional imports (should already exist in this notebook)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def _safe_cmd(cmd):\n",
    "    \"\"\"Run a command and return stdout; never crash the notebook.\"\"\"\n",
    "    try:\n",
    "        return subprocess.check_output(cmd, text=True, stderr=subprocess.STDOUT)\n",
    "    except Exception as e:\n",
    "        return f\"(command failed: {cmd} -> {e})\\n\"\n",
    "\n",
    "# --- Device snapshot ---\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "    except Exception:\n",
    "        gpu_name = \"(unknown GPU)\"\n",
    "    device_str = f\"cuda (GPU: {gpu_name})\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "\n",
    "# --- Attempt to detect an NRP/Jupyter image string (may be empty depending on platform config) ---\n",
    "image_hint = (\n",
    "    os.environ.get(\"JUPYTER_IMAGE_SPEC\")\n",
    "    or os.environ.get(\"CONTAINER_IMAGE\")\n",
    "    or os.environ.get(\"JUPYTERHUB_IMAGE\")\n",
    "    or os.environ.get(\"IMAGE_NAME\")\n",
    "    or \"\"\n",
    ")\n",
    "\n",
    "# --- Paths (repo root is assumed to be one level above the notebook directory) ---\n",
    "repo_root = Path(\"..\")\n",
    "env_path = repo_root / \"environment_summary.md\"\n",
    "freeze_path = repo_root / \"pip_freeze.txt\"\n",
    "\n",
    "# --- Build environment_summary.md content ---\n",
    "try:\n",
    "    import sklearn\n",
    "    sklearn_ver = sklearn.__version__\n",
    "except Exception:\n",
    "    sklearn_ver = \"(unavailable)\"\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    mpl_ver = matplotlib.__version__\n",
    "except Exception:\n",
    "    mpl_ver = \"(unavailable)\"\n",
    "\n",
    "env_md = f\"\"\"# environment_summary.md\n",
    "\n",
    "This file documents the execution environment required for reproducibility.\n",
    "\n",
    "## Required execution environment (course rule)\n",
    "Final results **must** be generated by running the notebooks end-to-end on **NRP JupyterHub** using the course-provided **PyTorch2** stack image.\n",
    "\n",
    "## Runtime snapshot\n",
    "- Generated: `{datetime.datetime.now().isoformat()}`\n",
    "- NRP image hint (auto-detected if available): `{image_hint if image_hint else \"UNKNOWN (fill in if your NRP image name is shown in JupyterHub)\"}`\n",
    "\n",
    "## System\n",
    "- Platform: `{platform.platform()}`\n",
    "- Python: `{sys.version.replace(chr(10), \" \")}`\n",
    "- Executable: `{sys.executable}`\n",
    "- Device: `{device_str}`\n",
    "- CUDA available: `{torch.cuda.is_available()}`\n",
    "- CUDA version (torch): `{torch.version.cuda}`\n",
    "\n",
    "## Key libraries\n",
    "- torch: `{torch.__version__}`\n",
    "- numpy: `{np.__version__}`\n",
    "- pandas: `{pd.__version__}`\n",
    "- scikit-learn: `{sklearn_ver}`\n",
    "- matplotlib: `{mpl_ver}`\n",
    "\n",
    "## Full dependency list (optional)\n",
    "See `pip_freeze.txt` (generated alongside this file).\n",
    "\"\"\"\n",
    "\n",
    "# --- Write files ---\n",
    "env_path.write_text(env_md, encoding=\"utf-8\")\n",
    "freeze_path.write_text(_safe_cmd([sys.executable, \"-m\", \"pip\", \"freeze\"]), encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Wrote:\", env_path.resolve())\n",
    "print(\"‚úÖ Wrote:\", freeze_path.resolve())\n",
    "print(\"Device:\", device_str)\n",
    "print(\"NRP image hint:\", image_hint if image_hint else \"(not detected)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf62d7",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 2 ‚Äî üß¨ Deterministic seeding across random number generators\n",
    "\n",
    "### üéØ Research aim\n",
    "Control stochasticity so that model comparisons reflect algorithmic choices rather than incidental randomness from initialization or minibatch ordering.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Define `seed_everything(seed)` to seed Python‚Äôs `random`, NumPy, and PyTorch RNG streams.\n",
    "- Configure CuDNN flags (`deterministic=True`, `benchmark=False`) to reduce nondeterministic kernel selection on GPUs.\n",
    "- Execute `seed_everything(SEED)` once to lock the run.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "A fixed seed turns pseudorandom draws into a reproducible sequence: if the data order and parameter initialization are functions of the seed, then repeated executions approximate the *same* stochastic process rather than different ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a9df1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Next section: compute the step below.\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1133ca7",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 3 ‚Äî üì• Local dataset load with bounded sampling and deterministic shuffle\n",
    "\n",
    "### üéØ Research aim\n",
    "Load a fixed-size subset of the dataset and makes sure that no redundancy is present.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Validate that `train.csv` exists at `DATA_PATH`; fail fast with a descriptive error if not.\n",
    "- Read `NROWS` rows into `df` using `pd.read_csv`.\n",
    "- Apply a seeded row permutation via `df.sample(frac=1.0, random_state=SEED)` and reset the index.\n",
    "- Display `df.head()` as a schema/column sanity check.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "Even before defining a network, this step influences optimization: minibatch gradient estimates can be biased when rows are ordered by collection time, vendor, or geography. A deterministic shuffle reduces that structured correlation.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "You should see the loaded DataFrame shape printed after shuffle. Use the preview to confirm the presence of coordinate, timestamp, and target columns.\n",
    "\n",
    "### ‚úÖ Quality checks\n",
    "If `Loaded:` shows fewer than `NROWS`, the file may be smaller than expected. If `pickup_datetime` or coordinate columns are missing, feature construction in later cells will fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f85e299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists. Skipping download.\n",
      "Loading dataset into memory...\n",
      "Loaded df: (1000000, 11)\n",
      "Shuffled with seed: 42\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Make sure the directory exists\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#If it does not exist then it will request to download from GitHub\n",
    "if not DATA_PATH.exists():\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, DATA_PATH)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download.\")\n",
    "\n",
    "print(\"Loading dataset into memory...\")\n",
    "# Load 1,000,000 rows\n",
    "df = pd.read_csv(DATA_PATH, nrows=NROWS)\n",
    "print(\"Loaded df:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# Next section: compute the step below.\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Shuffled with seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb5cde",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 4 ‚Äî ‚úÇÔ∏è Three-way partition: train, validation, and one-time test set\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Split `df` into `dev_df` and `holdout_df` with an **85/15** partition (the 15% split is the **final test set**).\n",
    "- Split `dev_df` into `train_df` and `val_df` so the **overall** proportions are **70% train / 15% validation**.\n",
    "- Print shapes for all three subsets to make the effective sample sizes explicit.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "The neural network will later be tuned using only `val_df`; early stopping and trial selection both depend on the validation partition created here. The test set (`holdout_df`) is conceptually treated as ‚Äúfuture data‚Äù and is touched **once** at the end.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "The printed `(rows, columns)` tuples indicate whether the splits match the intended proportions. These sizes contextualize the variance you should expect in R¬≤/RMSE estimates.\n",
    "\n",
    "### ‚úÖ Quality checks\n",
    "Confirm that `train_df` is roughly 70% of the dataset and that `val_df` and `holdout_df` are each roughly 15%. If proportions deviate, inspect the split arguments and random state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d43a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (700000, 11)\n",
      "val_df: (150000, 11)\n",
      "holdout_df (test): (150000, 11)\n"
     ]
    }
   ],
   "source": [
    "# Splits (overall):\n",
    "# - 70% Train\n",
    "# - 15% Validation\n",
    "# - 15% Test (stored in `holdout_df` and only used in the final comparison)\n",
    "\n",
    "# Split the dataset into development (train+val) and separate test set.\n",
    "dev_df, holdout_df = train_test_split(df, test_size=0.15, random_state=SEED)\n",
    "\n",
    "# Split dev into train/val so the overall proportions are 70/15 (i.e., val is 0.15/0.85 of dev).\n",
    "train_df, val_df = train_test_split(dev_df, test_size=(0.15/0.85), random_state=SEED)\n",
    "\n",
    "# Optional clarity alias (so later text can say \"test\" while keeping existing variables).\n",
    "test_df = holdout_df\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"val_df:\", val_df.shape)\n",
    "print(\"holdout_df (test):\", holdout_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329dafc",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 5 ‚Äî üß≠ Feature construction: temporal cycles, spatial geometry, and categorical proxies\n",
    "\n",
    "### üéØ Research aim\n",
    "Map raw taxi records into a numerical feature vector that encodes time structure, displacement, and operational context before any model is trained.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Implement `haversine_km` to compute great-circle distance between pickup and dropoff coordinates.\n",
    "- Define `build_features(df)` to assemble a feature matrix with:\n",
    "  - timestamp-derived components (`pickup_hour`, `pickup_dow`, `pickup_month`),\n",
    "  - cyclic encodings (`sin`/`cos`) for periodic variables,\n",
    "  - spatial deltas and `haversine_km`,\n",
    "  - proxies such as `passenger_count`, store-and-forward flag, and one-hot `vendor_id`.\n",
    "- Replace infinities and missing values with zeros to keep downstream linear algebra well-defined.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Two transformations are mathematically central here:\n",
    "1) **Cyclic encoding** embeds a periodic scalar \\(t\\) as \\((\\sin(2\\pi t/T),\\cos(2\\pi t/T))\\), avoiding artificial discontinuities (e.g., 23‚Üí0 hours).\n",
    "2) **Haversine distance** computes spherical arc length: \\(d = 2R\\arcsin(\\sqrt{a})\\), with \\(a = \\sin^2(\\Delta\\phi/2) + \\cos\\phi_1\\cos\\phi_2\\sin^2(\\Delta\\lambda/2)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a76dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Vectorized Haversine distance in km.\"\"\"\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "# Define helper function `build_features` used in later cells.\n",
    "def build_features(dfin: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(index=dfin.index)\n",
    "\n",
    "    # Temporal\n",
    "    dt = pd.to_datetime(dfin[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    X[\"pickup_hour\"]  = dt.dt.hour.fillna(0).astype(int)\n",
    "    X[\"pickup_dow\"]   = dt.dt.dayofweek.fillna(0).astype(int)\n",
    "    X[\"pickup_month\"] = dt.dt.month.fillna(0).astype(int)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    X[\"hour_sin\"] = np.sin(2*np.pi*X[\"pickup_hour\"]/24)\n",
    "    X[\"hour_cos\"] = np.cos(2*np.pi*X[\"pickup_hour\"]/24)\n",
    "    X[\"dow_sin\"]  = np.sin(2*np.pi*X[\"pickup_dow\"]/7)\n",
    "    X[\"dow_cos\"]  = np.cos(2*np.pi*X[\"pickup_dow\"]/7)\n",
    "\n",
    "    # Spatial\n",
    "    X[\"delta_lat\"] = (dfin[\"dropoff_latitude\"] - dfin[\"pickup_latitude\"]).astype(float)\n",
    "    X[\"delta_lon\"] = (dfin[\"dropoff_longitude\"] - dfin[\"pickup_longitude\"]).astype(float)\n",
    "    X[\"haversine_km\"] = haversine_km(\n",
    "        dfin[\"pickup_latitude\"].astype(float),\n",
    "        dfin[\"pickup_longitude\"].astype(float),\n",
    "        dfin[\"dropoff_latitude\"].astype(float),\n",
    "        dfin[\"dropoff_longitude\"].astype(float),\n",
    "    )\n",
    "\n",
    "    # Proxies\n",
    "    X[\"passenger_count\"] = pd.to_numeric(dfin[\"passenger_count\"], errors=\"coerce\").fillna(0.0)\n",
    "    X[\"store_and_fwd_Y\"] = (dfin[\"store_and_fwd_flag\"].astype(str).str.upper() == \"Y\").astype(int)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    vendor_oh = pd.get_dummies(dfin[\"vendor_id\"].astype(str), prefix=\"vendor\", drop_first=False)\n",
    "    X = pd.concat([X, vendor_oh], axis=1)\n",
    "\n",
    "    # Set global configuration/constants used throughout the notebook.\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e1448",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 6 ‚Äî üß© Feature matrix assembly with strict column alignment\n",
    "\n",
    "### üéØ Research aim\n",
    "Construct comparable design matrices for training and validation so that both the Ridge baseline and the neural network operate in the same coordinate system.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Compute `X_train = build_features(train_df)` and record its column set as `feature_cols`.\n",
    "- Build `X_val` and force it into the training column order via `reindex(columns=feature_cols, fill_value=0.0)`.\n",
    "- Extract numeric targets `y_train` and `y_val` as contiguous NumPy arrays (float64).\n",
    "- Print shapes to confirm dimensional compatibility.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "After one-hot encoding, the feature space is a basis indexed by column names. Reindexing ensures that both splits are represented in the same basis vector ordering, so an input row corresponds to a well-defined vector \\(x \\in \\mathbb{R}^d\\) with consistent semantics across datasets.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "The printed `X_train` and `X_val` shapes should match in the number of columns. Any mismatch indicates a feature-space inconsistency that would invalidate comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88143b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (700000, 14) X_val: (150000, 14)\n",
      "y_train: (700000,) y_val: (150000,)\n"
     ]
    }
   ],
   "source": [
    "# Build train/val features and ALIGN columns (critical for get_dummies)\n",
    "X_train = build_features(train_df)\n",
    "feature_cols = X_train.columns\n",
    "\n",
    "# Build engineered features and ensure train/val/holdout columns align.\n",
    "X_val = build_features(val_df).reindex(columns=feature_cols, fill_value=0.0)\n",
    "\n",
    "# Next section: compute the step below.\n",
    "y_train = train_df[TARGET].to_numpy().astype(np.float64)\n",
    "y_val   = val_df[TARGET].to_numpy().astype(np.float64)\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"X_train:\", X_train.shape, \"X_val:\", X_val.shape)\n",
    "print(\"y_train:\", y_train.shape, \"y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11376d5a",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 7 ‚Äî üìè Z-score standardization and persistence of preprocessing artifacts\n",
    "\n",
    "### üéØ Research aim\n",
    "Normalize feature scales using training statistics and store preprocessing parameters so that later evaluation reuses identical transformations.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Compute per-feature mean `mu` and standard deviation `sigma` on `X_train`.\n",
    "- Replace any zero standard deviations with 1 to avoid division by zero.\n",
    "- Standardize features: `X_train_s = (X_train - mu)/sigma` and apply the same map to `X_val`.\n",
    "- Save `mu`, `sigma`, and `feature_cols` into `ARTIFACTS_DIR` for reproducible inference.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Standardization applies \\(x' = (x-\\mu)/\\sigma\\) to the components. For Ridge regression, this prevents the L2 penalty from disproportionately shrinking coefficients simply because a feature is measured in larger units. For gradient methods, it moderates step-size sensitivity across dimensions.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "When inputs are standardized, early-layer gradients are less dominated by a single large-scale feature, which reduces the likelihood of unstable updates and helps Adam‚Äôs adaptive scaling behave as intended.\n",
    "\n",
    "### ‚úÖ Quality checks\n",
    "Inspect `sigma` for near-zero values (constant features). If many exist, consider whether those features should be removed; constant inputs contribute nothing but can complicate interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4d4e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts to: /home/jovyan/csusb_spring26_cse5140_team1/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Scale using TRAIN stats only\n",
    "mu = X_train.mean()\n",
    "sigma = X_train.std().replace(0, 1)\n",
    "\n",
    "# Apply standardization using training mean/std so models train stably.\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_val_s   = (X_val   - mu) / sigma\n",
    "\n",
    "# Save artifacts (optional)\n",
    "mu.to_csv(ARTIFACTS_DIR / \"mu.csv\")\n",
    "sigma.to_csv(ARTIFACTS_DIR / \"sigma.csv\")\n",
    "pd.Series(feature_cols, name=\"feature\").to_csv(ARTIFACTS_DIR / \"feature_cols.csv\", index=False)\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"Saved artifacts to:\", ARTIFACTS_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f3378",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 8 ‚Äî üßæ Evaluation utilities: log-space safety, inverse mapping, and metric bundle\n",
    "\n",
    "### üéØ Research aim\n",
    "Define a consistent evaluation interface that bridges the model‚Äôs log-space training objective and the project‚Äôs original-scale reporting requirements.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Fix a permissible log-prediction interval via `CLIP_MIN` and `CLIP_MAX`.\n",
    "- Implement `safe_expm1` to invert log predictions while enforcing the clip bounds.\n",
    "- Implement `mape` with an epsilon-protected denominator.\n",
    "- Implement `eval_regression` to compute R¬≤ and RMSE in log space, plus optional original-scale metrics (R¬≤/RMSE/MAE/MAPE).\n",
    "\n",
    "### üìê Mathematical lens\n",
    "The notebook models \\(y_{log} = \\log(1+y)\\) and inverts via \\(\\hat{y} = \\exp(\\hat{y}_{log})-1\\). Clipping constrains \\(\\hat{y}_{log}\\) to a bounded interval, which imposes a practical prior that extreme durations are not credible predictions. Metric-wise, \\(R^2 = 1 - \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2}\\) and RMSE summarizes typical error magnitude.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "The later MLP explicitly clamps its output in log space; these functions make that modeling choice measurable by reporting performance both where the model is trained (log) and where the task is interpreted (seconds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b9df73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLIP_MIN = -2.0\n",
    "CLIP_MAX = 13.0  # tighter cap; prevents absurd durations\n",
    "\n",
    "# Define helper function `safe_expm1` used in later cells.\n",
    "def safe_expm1(yhat_log, clip_min=CLIP_MIN, clip_max=CLIP_MAX):\n",
    "    yhat_log = np.asarray(yhat_log).reshape(-1)\n",
    "    yhat_log = np.clip(yhat_log, clip_min, clip_max)\n",
    "    return np.expm1(yhat_log)\n",
    "\n",
    "# Define helper function `mape` used in later cells.\n",
    "def mape(y_true, y_pred, eps=1.0):\n",
    "    y_true = np.asarray(y_true).reshape(-1).astype(np.float64)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1).astype(np.float64)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "# Define helper function `eval_regression` used in later cells.\n",
    "def eval_regression(y_true_log, y_pred_log, y_true_orig=None, label=\"\"):\n",
    "    \"\"\"Shared evaluation: log-space + optional original-scale metrics. Prints and returns dict.\"\"\"\n",
    "    y_pred_log = np.asarray(y_pred_log).reshape(-1)\n",
    "    y_true_log = np.asarray(y_true_log).reshape(-1)\n",
    "    metrics = {\n",
    "        \"R2_log\": r2_score(y_true_log, y_pred_log),\n",
    "        \"RMSE_log\": mean_squared_error(y_true_log, y_pred_log, squared=False),\n",
    "    }\n",
    "    if y_true_orig is not None:\n",
    "        y_pred_orig = safe_expm1(y_pred_log)\n",
    "        y_true_orig = np.asarray(y_true_orig).reshape(-1)\n",
    "        metrics[\"R2\"] = r2_score(y_true_orig, y_pred_orig)\n",
    "        metrics[\"RMSE\"] = mean_squared_error(y_true_orig, y_pred_orig, squared=False)\n",
    "        metrics[\"MAE\"] = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "        metrics[\"MAPE(%)\"] = mape(y_true_orig, y_pred_orig)\n",
    "    if label:\n",
    "        print(label)\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fabeb",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 9 ‚Äî üìê Ridge baseline: Œ± grid search on validation in log-target space\n",
    "\n",
    "### üéØ Research aim\n",
    "Construct a well-regularized linear comparator and select its regularization strength using validation performance as the decision criterion.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Transform targets to log space: `y_train_log = log1p(y_train)`, `y_val_log = log1p(y_val)`.\n",
    "- For each `alpha` in the grid, fit `Ridge(alpha=a)` on standardized features.\n",
    "- Predict log-duration on validation, apply clip bounds, invert to seconds, and compute multiple metrics.\n",
    "- Collect results in `ridge_grid_df`, select the best Œ± by validation R¬≤ (seconds), and recompute a consolidated metric report.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Ridge solves \\(\\min_w \\|y - Xw\\|_2^2 + \\alpha\\|w\\|_2^2\\). The penalty term shrinks coefficients toward zero continuously, which reduces variance when features are correlated (as temporal and spatial proxies often are). By fitting in log space, the linear model targets multiplicative effects and reduces skew-driven domination by extreme trips.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "This baseline anchors the hypothesis test: any NN improvement must be interpreted relative to a tuned linear model rather than an untuned or under-regularized comparator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d84d3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge alpha grid search (validation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2_log</th>\n",
       "      <th>RMSE_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.193161</td>\n",
       "      <td>6489.945708</td>\n",
       "      <td>478.178089</td>\n",
       "      <td>0.392262</td>\n",
       "      <td>0.617421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.193161</td>\n",
       "      <td>6489.945482</td>\n",
       "      <td>478.178074</td>\n",
       "      <td>0.392262</td>\n",
       "      <td>0.617421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.193160</td>\n",
       "      <td>6489.943224</td>\n",
       "      <td>478.177925</td>\n",
       "      <td>0.392261</td>\n",
       "      <td>0.617421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>-0.193151</td>\n",
       "      <td>6489.920647</td>\n",
       "      <td>478.176427</td>\n",
       "      <td>0.392260</td>\n",
       "      <td>0.617422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.00</td>\n",
       "      <td>-0.193069</td>\n",
       "      <td>6489.695104</td>\n",
       "      <td>478.161467</td>\n",
       "      <td>0.392242</td>\n",
       "      <td>0.617431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha        R2         RMSE         MAE    R2_log  RMSE_log\n",
       "0    0.01 -0.193161  6489.945708  478.178089  0.392262  0.617421\n",
       "1    0.10 -0.193161  6489.945482  478.178074  0.392262  0.617421\n",
       "2    1.00 -0.193160  6489.943224  478.177925  0.392261  0.617421\n",
       "3   10.00 -0.193151  6489.920647  478.176427  0.392260  0.617422\n",
       "4  100.00 -0.193069  6489.695104  478.161467  0.392242  0.617431"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best alpha = 100.0  (val R2 = -0.193069)\n",
      "\n",
      "VALIDATION ‚Äî Tuned Ridge:\n",
      "  R2_log: 0.39224186174901376\n",
      "  RMSE_log: 0.6174305785202495\n",
      "  R2: -0.1930685410856654\n",
      "  RMSE: 6489.695103815105\n",
      "  MAE: 478.16146723573604\n",
      "  MAPE(%): 71.06848352991541\n"
     ]
    }
   ],
   "source": [
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log   = np.log1p(y_val)\n",
    "\n",
    "# --- Ridge alpha grid search ---\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "ridge_results = []\n",
    "\n",
    "# Iterate through this section's loop to compute/update results.\n",
    "for a in alphas:\n",
    "    mdl = Ridge(alpha=a, random_state=SEED)\n",
    "    mdl.fit(X_train_s, y_train_log)\n",
    "    pred_log = np.clip(mdl.predict(X_val_s), CLIP_MIN, CLIP_MAX)\n",
    "    pred_orig = safe_expm1(pred_log)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    row = {\n",
    "        \"alpha\": a,\n",
    "        \"R2\": r2_score(y_val, pred_orig),\n",
    "        \"RMSE\": mean_squared_error(y_val, pred_orig, squared=False),\n",
    "        \"MAE\": mean_absolute_error(y_val, pred_orig),\n",
    "        \"R2_log\": r2_score(y_val_log, pred_log),\n",
    "        \"RMSE_log\": mean_squared_error(y_val_log, pred_log, squared=False),\n",
    "    }\n",
    "    ridge_results.append((mdl, row))\n",
    "\n",
    "# Next section: compute the step below.\n",
    "ridge_grid_df = pd.DataFrame([r for _, r in ridge_results])\n",
    "print(\"Ridge alpha grid search (validation):\")\n",
    "display(ridge_grid_df)\n",
    "\n",
    "# Select best alpha by validation R¬≤\n",
    "best_idx = ridge_grid_df[\"R2\"].idxmax()\n",
    "ridge, best_ridge_row = ridge_results[best_idx]\n",
    "print(f\"\\nBest alpha = {best_ridge_row['alpha']}  (val R2 = {best_ridge_row['R2']:.6f})\")\n",
    "\n",
    "# Recompute full validation metrics for best Ridge\n",
    "val_pred_log_ridge = np.clip(ridge.predict(X_val_s), CLIP_MIN, CLIP_MAX)\n",
    "baseline_val_metrics = eval_regression(y_val_log, val_pred_log_ridge, y_true_orig=y_val, label=\"\\nVALIDATION ‚Äî Tuned Ridge:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab3e7a",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 10 ‚Äî üß± PyTorch data interface: tabular dataset and minibatch loaders\n",
    "\n",
    "### üéØ Research aim\n",
    "Convert standardized arrays into PyTorch datasets so training can use efficient minibatch updates and consistent tensor dtypes.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Define `TabularDataset` to store `X` as `float32` tensors and `y_log` as a `(n,1)` tensor.\n",
    "- Implement `__len__` and `__getitem__` to satisfy the Dataset protocol.\n",
    "- Instantiate `train_ds` and `val_ds` from standardized matrices.\n",
    "- Create `DataLoader`s with shuffling enabled for training and disabled for validation.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Minibatching approximates the full-gradient objective with stochastic estimates: for batch \\(B\\), \\(\\nabla \\mathcal{L}_B(w)\\) serves as an estimator of \\(\\nabla \\mathcal{L}(w)\\). Shuffling changes the sampling scheme across epochs, reducing the chance that correlated rows create systematically biased gradient directions.\n",
    "\n",
    "### ‚úÖ Quality checks\n",
    "Verify that `X_train_s.values` and `y_train_log` have the same number of rows. If you later see size-mismatch errors in loss computation, this is the first place to check tensor shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d6feda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y_log):\n",
    "        self.X = torch.tensor(np.asarray(X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.asarray(y_log), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Define helper function `__len__` used in later cells.\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    # Define helper function `__getitem__` used in later cells.\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "# Next section: compute the step below.\n",
    "train_ds = TabularDataset(X_train_s.values, y_train_log)\n",
    "val_ds   = TabularDataset(X_val_s.values,   y_val_log)\n",
    "\n",
    "# Wrap arrays as PyTorch Datasets/DataLoaders for efficient mini-batch training.\n",
    "train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4096, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f64fb",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 11 ‚Äî üß† Randomized hyperparameter search for an MLP with early stopping\n",
    "\n",
    "### üéØ Research aim\n",
    "Train multiple candidate MLPs under controlled randomness, record their learning traces, and select the best configuration using validation performance only.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Set the number of trials `TRIALS` and create directories for plots and per-trial logs.\n",
    "- Define `build_mlp` to assemble an MLP with configurable hidden widths and dropout.\n",
    "- Define `evaluate_nn_log` for clipped log-space inference under `torch.no_grad()`.\n",
    "- Define `train_one_trial` implementing: per-trial seeding, Adam optimization, SmoothL1 loss, gradient clipping, validation RMSE_log tracking, and patience-based early stopping.\n",
    "- Sample configurations from `search_space`, run `TRIALS` trainings, select `best_model` by validation R¬≤, and persist results to `phase2_validation_results.csv`.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Several numerical choices here shape optimization:\n",
    "- **SmoothL1 (Huber) loss** transitions from quadratic to linear growth in the residual, reducing the influence of extreme errors.\n",
    "- **Weight decay** adds an L2 term to the objective, acting as parameter shrinkage in function space.\n",
    "- **Gradient clipping** enforces \\(\\|g\\|_2 \\le c\\), preventing rare large gradients from destabilizing Adam‚Äôs moment estimates.\n",
    "- **Early stopping** selects the parameter iterate with minimal validation RMSE_log, approximating model selection by estimated generalization.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "This is the notebook‚Äôs core neural methodology: a tabular MLP with dropout regularization and adaptive optimization. The random search strategy is intentional‚Äîin moderately sized spaces, sampling can discover good regions without exhaustive grids, while the stored histories enable post hoc diagnosis of training dynamics.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "Expect one printed line per trial reporting validation R¬≤/RMSE and the sampled configuration, followed by a saved CSV of trial results and a preview of the best-performing configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34248-973a-4fe4-91ee-81f7377d5800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space size: 6\n",
      "Example configs: [{'layers': (256, 128), 'dropout': 0.05, 'lr': 0.001, 'weight_decay': 1e-06, 'batch_size': 2048, 'epochs': 20, 'patience': 3}, {'layers': (256, 128), 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 1e-05, 'batch_size': 2048, 'epochs': 20, 'patience': 3}, {'layers': (512, 256), 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 1e-05, 'batch_size': 2048, 'epochs': 25, 'patience': 4}]\n",
      "Running 20 hyperparameter trials (validation-only selection)...\n",
      "Trial 01 | val_R2=0.0078 | val_RMSE=5918.30 | best_val_RMSE_log=0.4663 | cfg={'layers': (128, 64), 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 1e-05, 'batch_size': 1024, 'epochs': 20, 'patience': 3}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# 2.2 Hyperparameter Tuning Loop\n",
    "# -----------------------------\n",
    "TRIALS = 20  # set to 20‚Äì50\n",
    "\n",
    "# Set global configuration/constants used throughout the notebook.\n",
    "PLOTS_DIR = ARTIFACTS_DIR / \"plots\"\n",
    "LOG_DIR   = ARTIFACTS_DIR / \"trial_logs\"\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define helper function `build_mlp` used in later cells.\n",
    "def build_mlp(in_dim: int, layers=(256, 128), dropout=0.10):\n",
    "    net = []\n",
    "    prev = in_dim\n",
    "    for h in layers:\n",
    "        net.append(nn.Linear(prev, h))\n",
    "        net.append(nn.ReLU())\n",
    "        net.append(nn.Dropout(dropout))\n",
    "        prev = h\n",
    "    net.append(nn.Linear(prev, 1))\n",
    "    return nn.Sequential(*net).to(device)\n",
    "\n",
    "# Define helper function `evaluate_nn_log` used in later cells.\n",
    "def evaluate_nn_log(model, Xs_np):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_log = model(torch.tensor(Xs_np, dtype=torch.float32).to(device)).cpu().numpy().reshape(-1)\n",
    "    pred_log = np.clip(pred_log, CLIP_MIN, CLIP_MAX)\n",
    "    return pred_log\n",
    "\n",
    "# Define helper function `train_one_trial` used in later cells.\n",
    "def train_one_trial(trial_id: int, cfg: dict):\n",
    "    # Reset seeds per trial (prevents cross-trial contamination + improves reproducibility)\n",
    "    seed_everything(SEED + trial_id)\n",
    "\n",
    "    # Fresh model + optimizer per trial\n",
    "    model = build_mlp(in_dim=X_train_s.shape[1], layers=cfg[\"layers\"], dropout=cfg[\"dropout\"])\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    # Per-trial loaders (batch size can be tuned too)\n",
    "    train_ds = TabularDataset(X_train_s.values, y_train_log)\n",
    "    val_ds   = TabularDataset(X_val_s.values,   y_val_log)\n",
    "\n",
    "    # Wrap arrays as PyTorch Datasets/DataLoaders for efficient mini-batch training.\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4096, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Early stopping on validation RMSE in log-space\n",
    "    best_state = None\n",
    "    best_val_rmse_log = float(\"inf\")\n",
    "    patience = cfg[\"patience\"]\n",
    "    pat = 0\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    history = []\n",
    "    for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "\n",
    "        # Iterate through this section's loop to compute/update results.\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).view(-1)\n",
    "\n",
    "            # Reset gradients before the forward/backward pass for this batch.\n",
    "            optimizer.zero_grad()\n",
    "            pred_log = model(xb).view(-1)\n",
    "            pred_log = torch.clamp(pred_log, CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "            # Compute the training loss for this batch.\n",
    "            loss = loss_fn(pred_log, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Next section: compute the step below.\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "\n",
    "        # Next section: compute the step below.\n",
    "        train_loss = total / n\n",
    "\n",
    "        # Validate\n",
    "        val_pred_log = evaluate_nn_log(model, X_val_s.values.astype(np.float32))\n",
    "        val_rmse_log = mean_squared_error(y_val_log, val_pred_log, squared=False)\n",
    "\n",
    "        # Log per-epoch metrics so training curves can be plotted later.\n",
    "        history.append({\n",
    "            \"trial_id\": trial_id,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_rmse_log\": val_rmse_log,\n",
    "        })\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_rmse_log < best_val_rmse_log - 1e-4:\n",
    "            best_val_rmse_log = val_rmse_log\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                break\n",
    "\n",
    "    # Restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Final validation metrics (log + original scale)\n",
    "    val_pred_log = evaluate_nn_log(model, X_val_s.values.astype(np.float32))\n",
    "    val_pred     = safe_expm1(val_pred_log)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    val_rmse = mean_squared_error(y_val, val_pred, squared=False)\n",
    "    val_mae  = mean_absolute_error(y_val, val_pred)\n",
    "    val_r2   = r2_score(y_val, val_pred)\n",
    "\n",
    "    # Save per-epoch history for Brandon plots\n",
    "    hist_path = LOG_DIR / f\"trial_{trial_id}_history.csv\"\n",
    "    pd.DataFrame(history).to_csv(hist_path, index=False)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    result = {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"layers\": str(cfg[\"layers\"]),\n",
    "        \"dropout\": cfg[\"dropout\"],\n",
    "        \"lr\": cfg[\"lr\"],\n",
    "        \"weight_decay\": cfg[\"weight_decay\"],\n",
    "        \"batch_size\": cfg[\"batch_size\"],\n",
    "        \"epochs_cap\": cfg[\"epochs\"],\n",
    "        \"patience\": cfg[\"patience\"],\n",
    "        \"best_val_rmse_log\": best_val_rmse_log,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"history_csv\": str(hist_path),\n",
    "    }\n",
    "    return model, result\n",
    "\n",
    "# -----------------------------\n",
    "# Search space (simple + safe)\n",
    "# -----------------------------\n",
    "search_space = [\n",
    "    {\"layers\": (256,128), \"dropout\": 0.05, \"lr\": 1e-3, \"weight_decay\": 1e-6, \"batch_size\": 2048, \"epochs\": 20, \"patience\": 3},\n",
    "    {\"layers\": (256,128), \"dropout\": 0.10, \"lr\": 5e-4, \"weight_decay\": 1e-5, \"batch_size\": 2048, \"epochs\": 20, \"patience\": 3},\n",
    "    {\"layers\": (512,256), \"dropout\": 0.10, \"lr\": 5e-4, \"weight_decay\": 1e-5, \"batch_size\": 2048, \"epochs\": 25, \"patience\": 4},\n",
    "    {\"layers\": (512,256), \"dropout\": 0.20, \"lr\": 3e-4, \"weight_decay\": 1e-4, \"batch_size\": 2048, \"epochs\": 25, \"patience\": 4},\n",
    "    {\"layers\": (128,64),  \"dropout\": 0.05, \"lr\": 1e-3, \"weight_decay\": 1e-6, \"batch_size\": 1024, \"epochs\": 20, \"patience\": 3},\n",
    "    {\"layers\": (128,64),  \"dropout\": 0.10, \"lr\": 5e-4, \"weight_decay\": 1e-5, \"batch_size\": 1024, \"epochs\": 20, \"patience\": 3},\n",
    "]\n",
    "## This is to print safe space necessary for budget transparency accocrding to chatGPT\n",
    "print(\"Search space size:\", len(search_space))\n",
    "print(\"Example configs:\", search_space[:3])\n",
    "\n",
    "# If TRIALS > len(search_space), we'll sample with replacement\n",
    "seed_everything(SEED)\n",
    "\n",
    "# Next section: compute the step below.\n",
    "all_results = []\n",
    "best_model = None\n",
    "best_row = None\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(f\"Running {TRIALS} hyperparameter trials (validation-only selection)...\")\n",
    "\n",
    "#Start TIMER\n",
    "import time\n",
    "tuning_start = time.time()\n",
    "\n",
    "# Run multiple randomized trials and keep the best model by validation R¬≤.\n",
    "for t in range(1, TRIALS + 1):\n",
    "    cfg = random.choice(search_space)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    model, row = train_one_trial(t, cfg)\n",
    "    all_results.append(row)\n",
    "\n",
    "    # Print key configuration so runs are easy to reproduce/debug.\n",
    "    print(f\"Trial {t:02d} | val_R2={row['val_r2']:.4f} | val_RMSE={row['val_rmse']:.2f} | best_val_RMSE_log={row['best_val_rmse_log']:.4f} | cfg={cfg}\")\n",
    "\n",
    "    # Selection criterion: validation R¬≤ (strictly validation-based)\n",
    "    if best_row is None or row[\"val_r2\"] > best_row[\"val_r2\"]:\n",
    "        best_row = row\n",
    "        best_model = model\n",
    "\n",
    "# Persist preprocessing artifacts to disk for reuse and reproducibility.\n",
    "results_df = pd.DataFrame(all_results).sort_values(\"val_r2\", ascending=False)\n",
    "results_path = ARTIFACTS_DIR / \"phase2_validation_results.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"\\nSaved tuning results to:\", results_path.resolve())\n",
    "print(\"\\nBest trial selected by validation R¬≤:\")\n",
    "display(results_df.head(5))\n",
    "print(\"\\nBEST TRIAL:\", best_row)\n",
    "\n",
    "# END TIMER\n",
    "tuning_end = time.time()\n",
    "total_time = tuning_end - tuning_start\n",
    "\n",
    "print(\"\\n=== COMPUTE SUMMARY ===\")\n",
    "print(\"Trials run:\", TRIALS)\n",
    "print(\"Search space size:\", len(search_space))\n",
    "print(\"Total tuning time (seconds):\", round(total_time, 2))\n",
    "print(\"Total tuning time (minutes):\", round(total_time/60, 2))\n",
    "print(\"Device used:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae0c0b",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 11B ‚Äî üíæ Save + Reload the Selected Best Neural Network (Submission Artifact)\n",
    "\n",
    "### üéØ Research aim\n",
    "Persist the **selected best MLP** (chosen using **validation only**) so the grader can reload it and reproduce results **without retraining**.\n",
    "\n",
    "### ‚úÖ What this cell does\n",
    "- Saves a checkpoint to `../artifacts/best_mlp_model.pt` containing:\n",
    "  - MLP weights (`state_dict`)\n",
    "  - Architecture hyperparameters (layers, dropout, input dimension)\n",
    "  - Feature column order\n",
    "  - Training-set scaling (`mu`, `sigma`)\n",
    "  - Log-space clipping range (`CLIP_MIN`, `CLIP_MAX`)\n",
    "- Performs a **reload sanity check on the validation set** (allowed) to verify the saved file loads correctly.\n",
    "\n",
    "> We intentionally avoid re-evaluating the **test set** here to respect the ‚Äútest once‚Äù rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28eb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, json, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Where to save the model artifact (commit this file to the repo for grading) ---\n",
    "BEST_MODEL_PATH = ARTIFACTS_DIR / \"best_mlp_model.pt\"\n",
    "BEST_META_PATH  = ARTIFACTS_DIR / \"best_mlp_metadata.json\"\n",
    "\n",
    "# --- Build a clean, reloadable model configuration ---\n",
    "layers_raw = best_row.get(\"layers\", None)\n",
    "\n",
    "if isinstance(layers_raw, str):\n",
    "    try:\n",
    "        layers_tuple = tuple(ast.literal_eval(layers_raw))\n",
    "    except Exception:\n",
    "        # Fallback: extract integers from string like \"(256, 128)\"\n",
    "        import re\n",
    "        layers_tuple = tuple(int(x) for x in re.findall(r\"\\d+\", layers_raw))\n",
    "else:\n",
    "    layers_tuple = tuple(layers_raw) if layers_raw is not None else (256, 128)\n",
    "\n",
    "model_cfg = {\n",
    "    \"in_dim\": int(X_train_s.shape[1]),\n",
    "    \"layers\": tuple(int(x) for x in layers_tuple),\n",
    "    \"dropout\": float(best_row.get(\"dropout\", 0.10)),\n",
    "}\n",
    "\n",
    "# --- Save a portable checkpoint (CPU tensors) ---\n",
    "ckpt = {\n",
    "    \"model_cfg\": model_cfg,\n",
    "    \"state_dict\": {k: v.detach().cpu() for k, v in best_model.state_dict().items()},\n",
    "    \"feature_cols\": list(feature_cols),\n",
    "    \"mu\": mu.reindex(feature_cols).to_numpy().astype(np.float32),\n",
    "    \"sigma\": sigma.reindex(feature_cols).to_numpy().astype(np.float32),\n",
    "    \"clip_min\": float(CLIP_MIN),\n",
    "    \"clip_max\": float(CLIP_MAX),\n",
    "    \"seed\": int(SEED),\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"selection_note\": \"Best NN selected using validation only (no test leakage).\",\n",
    "}\n",
    "\n",
    "torch.save(ckpt, BEST_MODEL_PATH)\n",
    "\n",
    "# --- Also write a human-readable metadata JSON (useful for the report + grading) ---\n",
    "meta = {\n",
    "    \"saved_path\": str(BEST_MODEL_PATH),\n",
    "    \"timestamp\": ckpt[\"timestamp\"],\n",
    "    \"model_cfg\": ckpt[\"model_cfg\"],\n",
    "    \"n_features\": len(ckpt[\"feature_cols\"]),\n",
    "    \"clip_min\": ckpt[\"clip_min\"],\n",
    "    \"clip_max\": ckpt[\"clip_max\"],\n",
    "    \"seed\": ckpt[\"seed\"],\n",
    "    \"best_row_from_search\": {k: (float(v) if isinstance(v, (int, float, np.floating)) else v) for k, v in best_row.items()},\n",
    "}\n",
    "BEST_META_PATH.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Saved model checkpoint:\", BEST_MODEL_PATH.resolve())\n",
    "print(\"‚úÖ Saved metadata JSON:   \", BEST_META_PATH.resolve())\n",
    "print(\"Reload config:\", model_cfg)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reload sanity check on VALIDATION (allowed; avoids test reuse)\n",
    "# ------------------------------------------------------------\n",
    "loaded = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "\n",
    "reload_model = build_mlp(\n",
    "    in_dim=loaded[\"model_cfg\"][\"in_dim\"],\n",
    "    layers=tuple(loaded[\"model_cfg\"][\"layers\"]),\n",
    "    dropout=float(loaded[\"model_cfg\"][\"dropout\"]),\n",
    ")\n",
    "reload_model.load_state_dict(loaded[\"state_dict\"])\n",
    "reload_model.to(device).eval()\n",
    "\n",
    "val_pred_log_reload = evaluate_nn_log(reload_model, X_val_s.to_numpy().astype(np.float32))\n",
    "val_metrics_reload = eval_regression(\n",
    "    y_val_log,\n",
    "    val_pred_log_reload,\n",
    "    y_true_orig=y_val,\n",
    "    label=\"\\nRELOADED MODEL ‚Äî Neural Net (VALIDATION sanity check):\"\n",
    ")\n",
    "\n",
    "# Compare predictions to in-memory model on validation (should be identical or extremely close)\n",
    "val_pred_log_inmem = evaluate_nn_log(best_model, X_val_s.to_numpy().astype(np.float32))\n",
    "max_abs_diff = float(np.max(np.abs(val_pred_log_reload - val_pred_log_inmem)))\n",
    "print(\"\\nReload consistency check (validation predictions):\")\n",
    "print(\"  max |Œî pred_log| =\", max_abs_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978ecd4",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 12 ‚Äî üìà Trial-level performance scan: validation R¬≤ across configurations\n",
    "\n",
    "### üéØ Research aim\n",
    "Summarize the tuning experiment by visualizing how validation performance varies across trial instances.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Load `phase2_validation_results.csv` and order trials by `trial_id`.\n",
    "- Plot trial index against `val_r2` to reveal dispersion, trends, or outliers.\n",
    "- Label axes and add a grid for readable comparison.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "This plot is a visual proxy for sensitivity analysis: each point is an estimate of validation R¬≤ under a sampled hyperparameter configuration. Wide variance suggests that architecture/optimizer choices materially affect fit quality rather than producing marginal differences.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "Because each trial corresponds to a distinct MLP configuration, the curve indirectly reflects which hyperparameter regions are stable. It also flags whether performance improvements are sporadic (high variance) or systematic (consistent uplift).\n",
    "\n",
    "### üîç How to read the outputs\n",
    "A line plot with markers appears. Look for abrupt jumps or isolated peaks, which indicate that only a few configurations achieved strong validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7144b-c937-4569-abcd-13524bafb08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from CSV into a DataFrame for preprocessing and modeling.\n",
    "df = pd.read_csv(ARTIFACTS_DIR / \"phase2_validation_results.csv\").sort_values(\"trial_id\")\n",
    "\n",
    "# Visualize results to compare trials and diagnose training dynamics.\n",
    "plt.figure()\n",
    "plt.plot(df[\"trial_id\"], df[\"val_r2\"], marker=\"o\")\n",
    "plt.xlabel(\"Trial ID\")\n",
    "plt.ylabel(\"Validation R¬≤ (original scale)\")\n",
    "plt.title(\"Validation R¬≤ Across Trials\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613551e",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 13 ‚Äî üß™ Learning dynamics: training loss vs validation RMSE_log for top trials\n",
    "\n",
    "### üéØ Research aim\n",
    "Inspect how the strongest configurations *arrived* at their performance by comparing within-epoch optimization behavior to validation error trajectories.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Reload the tuning results and select the top 3 trials by `val_r2`.\n",
    "- For each selected trial, load its per-epoch `history_csv`.\n",
    "- Plot `train_loss` (SmoothL1) and `val_rmse_log` (dashed) across epochs on the same axes.\n",
    "- Use legends to keep trial identities distinguishable.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Two distinct functionals are plotted: the training objective (SmoothL1 on log residuals) and the validation metric (RMSE in log space). Their joint evolution indicates whether optimization is continuing to reduce training loss while validation error stabilizes or reverses.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "This diagnostic is particularly informative for dropout/weight-decay regimes: strong regularization often yields slower training loss reduction but smoother validation curves, whereas unstable learning rates manifest as oscillatory validation RMSE_log.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "Expect multiple curves: for each trial, a solid line for training loss and a dashed line for validation RMSE_log. Early-stopped trials will have shorter traces.\n",
    "\n",
    "### ‚úÖ Quality checks\n",
    "If `val_rmse_log` increases while `train_loss` decreases monotonically, the configuration may be over-specializing to training minibatches. If both plateau early, capacity or learning rate may be limiting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985b776-8269-40a1-a564-1e79a9282875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from CSV into a DataFrame for preprocessing and modeling.\n",
    "df = pd.read_csv(ARTIFACTS_DIR / \"phase2_validation_results.csv\")\n",
    "top = df.sort_values(\"val_r2\", ascending=False).head(3)\n",
    "\n",
    "# Visualize results to compare trials and diagnose training dynamics.\n",
    "plt.figure()\n",
    "for _, row in top.iterrows():\n",
    "    hist = pd.read_csv(row[\"history_csv\"])\n",
    "    plt.plot(hist[\"epoch\"], hist[\"train_loss\"], label=f\"Trial {int(row['trial_id'])} train\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_rmse_log\"], linestyle=\"--\", label=f\"Trial {int(row['trial_id'])} val_rmse_log\")\n",
    "\n",
    "# Visualize results to compare trials and diagnose training dynamics.\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss / RMSE_log\")\n",
    "plt.title(\"Training Loss vs Validation RMSE_log (Top Trials)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5906a75",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 14 ‚Äî üîç Compact summary: end-of-training statistics for high-performing trials\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Select the top 5 trials by validation R¬≤.\n",
    "- For each, read the trial‚Äôs history file and extract the final epoch row.\n",
    "- Record `final_train_loss` and `final_val_rmse_log` alongside `val_r2`.\n",
    "- Assemble `gap_df` and sort it by `val_r2` for quick ranking.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "Where the curve plot emphasizes *trajectory*, this table emphasizes *endpoint state*. Together they form a minimal audit trail: which trial won, and what its terminal training/validation behavior looked like.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "A DataFrame appears with columns such as `trial_id`, `val_r2`, `final_train_loss`, and `final_val_rmse_log`. Use it to spot trials whose endpoint statistics look inconsistent with their rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5a0ad-97d1-43e5-a39f-f30de899725d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV into a DataFrame for preprocessing and modeling.\n",
    "df = pd.read_csv(ARTIFACTS_DIR / \"phase2_validation_results.csv\")\n",
    "top = df.sort_values(\"val_r2\", ascending=False).head(5)\n",
    "\n",
    "# Display: sets a table with coparing the top trials.\n",
    "rows = []\n",
    "for _, r in top.iterrows():\n",
    "    hist = pd.read_csv(r[\"history_csv\"])\n",
    "    final = hist.iloc[-1]\n",
    "    rows.append({\n",
    "        \"trial_id\": int(r[\"trial_id\"]),\n",
    "        \"val_r2\": r[\"val_r2\"],\n",
    "        \"final_train_loss\": final[\"train_loss\"],\n",
    "        \"final_val_rmse_log\": final[\"val_rmse_log\"],\n",
    "    })\n",
    "\n",
    "# Output: displays the table.\n",
    "gap_df = pd.DataFrame(rows).sort_values(\"val_r2\", ascending=False)\n",
    "gap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49998e",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 15 ‚Äî ‚úÖ Validation report for the selected best MLP\n",
    "\n",
    "### üéØ Research aim\n",
    "Generate a single, consolidated validation evaluation for the chosen neural network and verify that predictions remain within the intended log-domain.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Use `evaluate_nn_log(best_model, X_val_s)` to obtain clipped log predictions.\n",
    "- Print the min/max of `val_pred_log_nn` as a boundary diagnostic.\n",
    "- Compute the full metric bundle with `eval_regression(...)` and store it in `nn_val_metrics`.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "The min/max probe checks whether outputs concentrate at the clipping bounds. If many predictions equal `CLIP_MIN` or `CLIP_MAX`, the effective model becomes partially saturated, and improvements in R¬≤ may be constrained by this imposed range rather than by representational limits.\n",
    "\n",
    "### üß† Neural-network connection\n",
    "This cell operationalizes the model-selection decision: it evaluates the exact `best_model` object produced during tuning, rather than a hypothetical configuration. In other words, the reported validation metrics correspond to the network weights restored from the best early-stopping checkpoint.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "Expect two printed scalars (max/min log prediction) and a labeled metric block. These numbers are the direct validation-side evidence used before touching the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN validation metrics (log space + original scale)\n",
    "val_pred_log_nn = evaluate_nn_log(best_model, X_val_s.values.astype(np.float32))\n",
    "\n",
    "# Print key configuration so runs are easy to reproduce/debug.\n",
    "print(\"Max log prediction:\", float(np.max(val_pred_log_nn)))\n",
    "print(\"Min log prediction:\", float(np.min(val_pred_log_nn)))\n",
    "\n",
    "# Next section: compute the step below.\n",
    "nn_val_metrics = eval_regression(y_val_log, val_pred_log_nn, y_true_orig=y_val, label=\"VALIDATION ‚Äî Neural Net:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca64a1",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 16 ‚Äî üß≠ Permutation importance on validation (R¬≤_log drop) for Ridge vs MLP\n",
    "\n",
    "### üéØ Research aim\n",
    "Quantify how much each engineered feature contributes to predictive performance by measuring performance degradation under controlled feature perturbations.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- Define `permutation_importance_r2log`:\n",
    "  1) compute a baseline `R2_log`,\n",
    "  2) permute one feature column repeatedly,\n",
    "  3) recompute `R2_log`, and\n",
    "  4) record the mean and standard deviation of the drop.\n",
    "- Implement `ridge_predict_log_fn` and `nn_predict_log_fn` so both models are evaluated in the same scaled feature space.\n",
    "- Compute and display the top-ranked features for Ridge and for the neural network.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Permutation importance estimates \\(\\Delta R^2_{log}(j) = R^2_{base} - R^2_{perm(j)}\\). It approximates the reliance of the predictor on feature \\(j\\) under the perturbation distribution induced by random permutation. Because permutation breaks joint dependence structure, highly correlated features can ‚Äòshare‚Äô importance and reduce each other‚Äôs apparent \\(\\Delta R^2\\).\n",
    "\n",
    "### üß† Neural-network connection\n",
    "Comparing Ridge vs MLP importances distinguishes linear dependence from nonlinear utilization: a feature that matters to the MLP but not Ridge may be influential through interactions, whereas agreement across both models suggests a stable, representation-robust signal.\n",
    "\n",
    "### üîç How to read the outputs\n",
    "Two ranked tables are displayed (top ~15 features each) along with baseline `R2_log` printouts. Focus on the top few drops; the tail often reflects noise-level sensitivity.\n",
    "\n",
    "### ‚ö†Ô∏è Stability + correlation note (interpretation)\n",
    "Permutation importance can be **unstable** when features are highly correlated (e.g., multiple distance/time proxies). In that case, importance may be ‚Äúshared‚Äù across correlated inputs, so a single feature‚Äôs drop may look smaller than expected.\n",
    "\n",
    "To mitigate this in reporting:\n",
    "- Interpret results at the **feature-family** level (temporal vs distance vs passenger/context), not only individual columns.\n",
    "- (Optional) rerun permutation importance with a few different random seeds / bootstrap samples and report a mean ¬± std to show stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf214d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define helper function `permutation_importance_r2log` used in later cells.\n",
    "def permutation_importance_r2log(predict_log_fn, X_df, y_true_log, n_repeats=5, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    base_pred_log = predict_log_fn(X_df)\n",
    "    base_r2 = r2_score(y_true_log, base_pred_log)\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    importances = []\n",
    "    X_work = X_df.copy()\n",
    "\n",
    "    # Iterate through this section's loop to compute/update results.\n",
    "    for col in X_df.columns:\n",
    "        drops = []\n",
    "        for _ in range(n_repeats):\n",
    "            saved = X_work[col].to_numpy().copy()\n",
    "            X_work[col] = rng.permutation(X_work[col].to_numpy())\n",
    "            pred_log = predict_log_fn(X_work)\n",
    "            drops.append(base_r2 - r2_score(y_true_log, pred_log))\n",
    "            X_work[col] = saved\n",
    "        importances.append((col, float(np.mean(drops)), float(np.std(drops))))\n",
    "\n",
    "    # Next section: compute the step below.\n",
    "    imp = (pd.DataFrame(importances, columns=[\"feature\", \"mean_r2log_drop\", \"std_r2log_drop\"])\n",
    "           .sort_values(\"mean_r2log_drop\", ascending=False)\n",
    "           .reset_index(drop=True))\n",
    "    return imp, base_r2\n",
    "\n",
    "# Ridge predict (log space) ‚Äî keep DataFrame to preserve feature names\n",
    "def ridge_predict_log_fn(Xdf_raw):\n",
    "    Xs_df = (Xdf_raw - mu) / sigma\n",
    "    pred_log = ridge.predict(Xs_df)\n",
    "    return np.clip(pred_log, CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "# NN predict (log space)\n",
    "def nn_predict_log_fn(Xdf_raw):\n",
    "    Xs = ((Xdf_raw - mu) / sigma).to_numpy().astype(np.float32)\n",
    "    return evaluate_nn_log(best_model, Xs)\n",
    "\n",
    "# Next section: compute the step below.\n",
    "imp_ridge, ridge_r2log = permutation_importance_r2log(ridge_predict_log_fn, X_val, y_val_log, n_repeats=5, seed=SEED)\n",
    "print(\"Permutation importance ‚Äî Tuned Ridge on VAL (log space)\")\n",
    "print(\"Baseline VAL R2_log:\", ridge_r2log)\n",
    "display(imp_ridge.head(15))\n",
    "\n",
    "# Next section: compute the step below.\n",
    "imp_nn, nn_r2log = permutation_importance_r2log(nn_predict_log_fn, X_val, y_val_log, n_repeats=5, seed=SEED)\n",
    "print(\"\\nPermutation importance ‚Äî Neural Net on VAL (log space)\")\n",
    "print(\"NN VAL R2_log:\", nn_r2log)\n",
    "display(imp_nn.head(15))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 9 (Submission requirement): Grouped feature-importance table\n",
    "# ------------------------------------------------------------\n",
    "# We add a human-readable \"group\" label (Temporal / Spatial / Context) and\n",
    "# present a combined table + a grouped summary. This improves interpretability.\n",
    "\n",
    "def feature_group(name: str) -> str:\n",
    "    # Temporal\n",
    "    if name.startswith(\"pickup_\") or name in {\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"}:\n",
    "        return \"Temporal\"\n",
    "    # Spatial / geometry\n",
    "    if name in {\"delta_lat\",\"delta_lon\",\"haversine_km\"}:\n",
    "        return \"Spatial\"\n",
    "    # Context / categorical proxies\n",
    "    if name in {\"passenger_count\",\"store_and_fwd_Y\"}:\n",
    "        return \"Context\"\n",
    "    if name.startswith(\"vendor_\"):\n",
    "        return \"Context\"\n",
    "    return \"Other\"\n",
    "\n",
    "def add_group_cols(imp_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    df = imp_df.copy()\n",
    "    df[\"group\"] = df[\"feature\"].map(feature_group)\n",
    "    df[\"model\"] = model_name\n",
    "    return df\n",
    "\n",
    "imp_ridge_g = add_group_cols(imp_ridge, \"Tuned Ridge\")\n",
    "imp_nn_g    = add_group_cols(imp_nn,    \"Best NN\")\n",
    "\n",
    "TOPK = 15\n",
    "grouped_top = pd.concat([imp_ridge_g.head(TOPK), imp_nn_g.head(TOPK)], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== Step 9 ‚Äî Grouped Feature Importance (Top 15 features for each model) ===\")\n",
    "display(grouped_top[[\"model\",\"group\",\"feature\",\"mean_r2log_drop\",\"std_r2log_drop\"]])\n",
    "\n",
    "# Group-level summary (heuristic): sums of mean drops within each group.\n",
    "# IMPORTANT: permutation drops are not strictly additive under collinearity;\n",
    "# this summary is mainly for readability.\n",
    "group_summary = (\n",
    "    pd.concat([imp_ridge_g, imp_nn_g], ignore_index=True)\n",
    "      .groupby([\"model\",\"group\"])\n",
    "      .agg(\n",
    "          n_features=(\"feature\",\"count\"),\n",
    "          total_mean_r2log_drop=(\"mean_r2log_drop\",\"sum\"),\n",
    "          mean_std_r2log_drop=(\"std_r2log_drop\",\"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"model\",\"total_mean_r2log_drop\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(\"\\nGrouped summary (heuristic readability aid):\")\n",
    "display(group_summary)\n",
    "\n",
    "print(\"Note: Permutation-importance drops are not strictly additive when features are correlated; group totals are for interpretability, not a conserved quantity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65eedf6",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Why Log R¬≤ Is Our Primary Evaluation Metric\n",
    "\n",
    "### üìê The Problem: Skewed Trip Durations\n",
    "\n",
    "NYC taxi trip durations are **heavily right-skewed** ‚Äî the vast majority of trips are short (5‚Äì15 minutes), but a long tail of trips extends to several hours. This skew causes problems for standard regression metrics:\n",
    "\n",
    "- **RMSE and R¬≤ on raw durations** are dominated by a small number of extreme outliers.\n",
    "- A model can appear to perform poorly (even negative R¬≤) simply because a few very long trips produce enormous squared errors.\n",
    "\n",
    "### üîÑ The Solution: Log Transform\n",
    "\n",
    "By applying `log1p(trip_duration)` we:\n",
    "\n",
    "1. **Compress the long tail** ‚Äî extreme values are pulled closer to the mean.\n",
    "2. **Stabilize variance** ‚Äî prediction errors become more uniform across short and long trips.\n",
    "3. **Align with proportional accuracy** ‚Äî a 2-minute error on a 10-minute trip matters more than a 2-minute error on a 2-hour trip. Log space naturally captures this.\n",
    "\n",
    "### üìä What This Means for Our Metrics\n",
    "\n",
    "| Metric | Space | Role in Our Evaluation |\n",
    "|--------|-------|------------------------|\n",
    "| **R¬≤_log** | Log-transformed | **Primary metric** ‚Äî fair comparison across all trip lengths |\n",
    "| **RMSE_log** | Log-transformed | Complementary ‚Äî measures average log-space error |\n",
    "| **R¬≤** | Original (seconds) | Reported for transparency ‚Äî may be negative due to skew |\n",
    "| **RMSE** | Original (seconds) | Reported for transparency ‚Äî sensitive to outliers |\n",
    "| **MAPE** | Original (seconds) | Interpretable percentage error |\n",
    "\n",
    "### ‚ö†Ô∏è Why Original-Scale R¬≤ Can Be Negative\n",
    "\n",
    "A negative R¬≤ on original-scale durations does **not** mean the model is useless. It means the model's squared errors (in seconds) exceed the variance of the raw target ‚Äî which is inflated by extreme outliers. The log-space R¬≤ and MAPE provide a much more reliable picture of predictive quality.\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "We report **all metrics** for full transparency, but use **R¬≤_log as the primary comparison metric** because it:\n",
    "- Is robust to the heavy-tailed distribution\n",
    "- Fairly evaluates both short and long trips\n",
    "- Aligns with standard practice for skewed regression targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ca115",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Cell 17 ‚Äî üèÅ Final test scorecard (Ridge vs MLP)\n",
    "\n",
    "### üéØ Research aim\n",
    "Apply the already-selected models and preprocessing maps to the reserved **test** split (stored in `holdout_df`), then report results in a single comparison table.\n",
    "\n",
    "### üßÆ Computational steps\n",
    "- **Load the saved NN checkpoint** (`../artifacts/best_mlp_model.pt`) so the final test score corresponds to the persisted submission artifact.\n",
    "- Construct `X_holdout` with the same feature function and the **saved feature column order** from the checkpoint.\n",
    "- Form `y_holdout` and `y_holdout_log` to match the evaluation convention used throughout the notebook.\n",
    "- Standardize `X_holdout` using the **training-derived** `(mu, sigma)` stored in the checkpoint (no refitting).\n",
    "- Produce **test-set** predictions for Ridge and the **reloaded** MLP, compute metrics via `eval_regression`, and compile them into `summary`.\n",
    "\n",
    "### üìê Mathematical lens\n",
    "Viewed as function composition, each model evaluates \\(\\hat{y} = h( s( f(r) ) )\\), where \\(f\\) is feature construction, \\(s\\) is standardization, and \\(h\\) is the predictor (Ridge or MLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420827db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build test features ONLY here (one-time test evaluation)\n",
    "\n",
    "# Load the saved NN checkpoint so the test results correspond to the persisted artifact\n",
    "BEST_MODEL_PATH = ARTIFACTS_DIR / \"best_mlp_model.pt\"\n",
    "assert BEST_MODEL_PATH.exists(), f\"Missing model file: {BEST_MODEL_PATH}. Run the save-model cell first.\"\n",
    "\n",
    "ckpt = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "saved_feature_cols = ckpt[\"feature_cols\"]\n",
    "\n",
    "# Restore scaling from the checkpoint (ensures inference matches training exactly)\n",
    "mu_saved = pd.Series(ckpt[\"mu\"], index=saved_feature_cols)\n",
    "sigma_saved = pd.Series(ckpt[\"sigma\"], index=saved_feature_cols)\n",
    "\n",
    "# Build test features using the saved feature ordering\n",
    "X_holdout = build_features(holdout_df).reindex(columns=saved_feature_cols, fill_value=0.0)\n",
    "y_holdout = holdout_df[TARGET].to_numpy().astype(np.float64)\n",
    "y_holdout_log = np.log1p(y_holdout)\n",
    "\n",
    "# Apply standardization using TRAIN stats only (from checkpoint)\n",
    "X_holdout_s = (X_holdout - mu_saved) / sigma_saved\n",
    "\n",
    "# -------------------------\n",
    "# Baseline test (Tuned Ridge)\n",
    "# -------------------------\n",
    "hold_pred_log_ridge = ridge.predict(X_holdout_s)\n",
    "hold_pred_log_ridge = np.clip(hold_pred_log_ridge, CLIP_MIN, CLIP_MAX)\n",
    "hold_ridge_metrics = eval_regression(\n",
    "    y_holdout_log,\n",
    "    hold_pred_log_ridge,\n",
    "    y_true_orig=y_holdout,\n",
    "    label=\"TEST ‚Äî Tuned Ridge:\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# NN test (Reloaded Best NN)\n",
    "# -------------------------\n",
    "best_nn_loaded = build_mlp(\n",
    "    in_dim=ckpt[\"model_cfg\"][\"in_dim\"],\n",
    "    layers=tuple(ckpt[\"model_cfg\"][\"layers\"]),\n",
    "    dropout=float(ckpt[\"model_cfg\"][\"dropout\"]),\n",
    ")\n",
    "best_nn_loaded.load_state_dict(ckpt[\"state_dict\"])\n",
    "best_nn_loaded.to(device).eval()\n",
    "\n",
    "hold_pred_log_nn = evaluate_nn_log(best_nn_loaded, X_holdout_s.to_numpy().astype(np.float32))\n",
    "hold_nn_metrics = eval_regression(\n",
    "    y_holdout_log,\n",
    "    hold_pred_log_nn,\n",
    "    y_true_orig=y_holdout,\n",
    "    label=\"\\nTEST ‚Äî Neural Net (reloaded checkpoint):\"\n",
    ")\n",
    "\n",
    "# --- Summary table ---\n",
    "summary = pd.DataFrame([\n",
    "    {\"Model\": \"Tuned Ridge\", **hold_ridge_metrics},\n",
    "    {\"Model\": \"Best NN\", **hold_nn_metrics},\n",
    "])\n",
    "\n",
    "print(\"\\n=== Test Summary (Final Comparison) ===\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "N_BOOTSTRAP = 1_000\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "n = len(y_holdout_log)\n",
    "\n",
    "delta_r2_log_samples = []\n",
    "delta_mape_samples = []\n",
    "\n",
    "for i in range(N_BOOTSTRAP):\n",
    "    idx = rng.randint(0, n, size=n)  # bootstrap resample indices\n",
    "\n",
    "    # Resample true values\n",
    "    y_true_log_b = y_holdout_log[idx]\n",
    "    y_true_orig_b = y_holdout[idx]\n",
    "\n",
    "    # Resample predictions\n",
    "    nn_pred_log_b = hold_pred_log_nn[idx]\n",
    "    ridge_pred_log_b = hold_pred_log_ridge[idx]\n",
    "\n",
    "    # R¬≤_log for both models\n",
    "    r2_nn = r2_score(y_true_log_b, nn_pred_log_b)\n",
    "    r2_ridge = r2_score(y_true_log_b, ridge_pred_log_b)\n",
    "    delta_r2_log_samples.append(r2_nn - r2_ridge)\n",
    "\n",
    "    # MAPE for both models (original scale)\n",
    "    mape_nn = mape(y_true_orig_b, safe_expm1(nn_pred_log_b))\n",
    "    mape_ridge = mape(y_true_orig_b, safe_expm1(ridge_pred_log_b))\n",
    "    delta_mape_samples.append(mape_nn - mape_ridge)\n",
    "\n",
    "delta_r2_log_samples = np.array(delta_r2_log_samples)\n",
    "delta_mape_samples = np.array(delta_mape_samples)\n",
    "\n",
    "# 95% Confidence Intervals\n",
    "ci_r2 = np.percentile(delta_r2_log_samples, [2.5, 97.5])\n",
    "ci_mape = np.percentile(delta_mape_samples, [2.5, 97.5])\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä Bootstrap Statistical Validation on TEST (1,000 resamples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nŒîR¬≤_log (NN ‚àí Ridge):\")\n",
    "print(f\"  Mean:   {delta_r2_log_samples.mean():.6f}\")\n",
    "print(f\"  95% CI: [{ci_r2[0]:.6f}, {ci_r2[1]:.6f}]\")\n",
    "if ci_r2[0] > 0:\n",
    "    print(\"  ‚úÖ 0 is NOT in the interval ‚Üí NN improvement is statistically significant\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è 0 is inside the interval ‚Üí NN improvement is NOT statistically significant\")\n",
    "\n",
    "print(f\"\\nŒîMAPE (NN ‚àí Ridge):\")\n",
    "print(f\"  Mean:   {delta_mape_samples.mean():.4f}%\")\n",
    "print(f\"  95% CI: [{ci_mape[0]:.4f}%, {ci_mape[1]:.4f}%]\")\n",
    "if ci_mape[1] < 0:\n",
    "    print(\"  ‚úÖ 0 is NOT in the interval ‚Üí NN has significantly lower MAPE (better)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è 0 is inside the interval ‚Üí MAPE difference is NOT statistically significant\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35018f9e",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Limitations (Phase 1)\n",
    "\n",
    "Even with strict reproducibility and a fair 70/15/15 split, several limitations remain:\n",
    "\n",
    "- **Heavy-tailed target distribution:** Trip duration contains extreme outliers (e.g., traffic, detours, errors). These can dominate original-scale squared-error metrics and can make original-scale **R¬≤** appear negative even when predictions are useful in typical cases.\n",
    "- **Clipping in log-space:** We clip predictions to [`CLIP_MIN`, `CLIP_MAX`] to avoid absurd values. This is practical for stability, but it also means very rare long trips are effectively ‚Äúcapped‚Äù by design.\n",
    "- **Limited feature set:** We only use a small set of engineered temporal/spatial/context features from `train.csv`. Without external context (weather, events, road networks), there is an upper bound on achievable accuracy.\n",
    "- **Correlated features and importance uncertainty:** Permutation importance can understate the importance of correlated features because ‚Äúshared signal‚Äù can be redistributed across multiple columns (especially in the vendor + time + geometry space).\n",
    "- **Finite tuning budget:** The NN search is constrained to a fixed number of trials and a small safe search space; a better configuration may exist but was not explored under the Phase 1 compute budget.\n",
    "\n",
    "### How Phase 2 can address these\n",
    "- **Evolutionary Algorithms (EA)** can perform a more systematic architecture/hyperparameter search under a fixed budget and report optimization cost explicitly.\n",
    "- **Fuzzy Systems (FS)** can provide a more interpretable model (limited feature set + explicit rules), making trade-offs between accuracy and interpretability clearer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec896f-cf0d-416d-9f7f-2874a93882bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Derived test-set improvements (NN - Ridge)\n",
    "delta_r2_log_test = hold_nn_metrics[\"R2_log\"] - hold_ridge_metrics[\"R2_log\"]\n",
    "delta_mape_test   = hold_nn_metrics[\"MAPE(%)\"] - hold_ridge_metrics[\"MAPE(%)\"]  # negative = better\n",
    "\n",
    "# Helpful boolean flags (from bootstrap CIs computed above)\n",
    "r2_improvement_supported   = (ci_r2[0] > 0)\n",
    "mape_improvement_supported = (ci_mape[1] < 0)\n",
    "\n",
    "# Optional: summarize feature-importance groups if available\n",
    "try:\n",
    "    nn_group_ranking = (group_summary[group_summary[\"model\"] == \"Best NN\"]\n",
    "                        .sort_values(\"total_mean_r2log_drop\", ascending=False)\n",
    "                        .reset_index(drop=True))\n",
    "    top_group_line = f\"{nn_group_ranking.loc[0, 'group']} (total mean drop ‚âà {nn_group_ranking.loc[0, 'total_mean_r2log_drop']:.4f})\"\n",
    "except Exception:\n",
    "    top_group_line = \"(group summary not available)\"\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "---\n",
    "# Conclusion (Phase 1 ‚Äî Neural Networks)\n",
    "\n",
    "## 1) Protocol summary (fair split + no leakage)\n",
    "\n",
    "- **Data used:** first **{NROWS:,} rows** from `train.csv`, deterministically **shuffled with seed {SEED}**.\n",
    "- **Split rule (fairness requirement):**  \n",
    "  - Train: **{len(train_df):,}** rows (‚âà70%)  \n",
    "  - Validation: **{len(val_df):,}** rows (‚âà15%)  \n",
    "  - Test: **{len(holdout_df):,}** rows (‚âà15%)  \n",
    "- **Leakage discipline:**  \n",
    "  - Validation is used for **tuning** (Ridge Œ± + NN hyperparameters / early stopping).  \n",
    "  - The **test set is evaluated exactly once** to produce final numbers.\n",
    "\n",
    "## 2) Final test-set results (Ridge vs Neural Network)\n",
    "\n",
    "On the **test set**, the tuned neural network outperforms the tuned Ridge baseline:\n",
    "\n",
    "- **Tuned Ridge (test):**\n",
    "  - **R¬≤_log = {hold_ridge_metrics['R2_log']:.4f}**\n",
    "  - **MAPE = {hold_ridge_metrics['MAPE(%)']:.2f}%**\n",
    "\n",
    "- **Best NN (test):**\n",
    "  - **R¬≤_log = {hold_nn_metrics['R2_log']:.4f}**\n",
    "  - **MAPE = {hold_nn_metrics['MAPE(%)']:.2f}%**\n",
    "\n",
    "**Observed improvement (NN ‚àí Ridge):**\n",
    "- **ŒîR¬≤_log = {delta_r2_log_test:.4f}**\n",
    "- **ŒîMAPE = {delta_mape_test:.2f}%** *(negative means the NN has lower error ‚Üí better)*\n",
    "\n",
    "## 3) Statistical validation (showing the gain is not ‚Äúluck‚Äù)\n",
    "\n",
    "We used **bootstrap resampling (N = {N_BOOTSTRAP:,}) on the test set** to estimate the uncertainty of the improvement:\n",
    "\n",
    "- **95% CI for ŒîR¬≤_log:** **[{ci_r2[0]:.6f}, {ci_r2[1]:.6f}]**  \n",
    "  - Interpretation: **{\"0 is excluded ‚Üí improvement is supported\" if r2_improvement_supported else \"0 is included ‚Üí improvement is not statistically supported\"}**\n",
    "- **95% CI for ŒîMAPE:** **[{ci_mape[0]:.4f}%, {ci_mape[1]:.4f}%]**  \n",
    "  - Interpretation: **{\"0 is excluded ‚Üí NN MAPE is significantly lower\" if mape_improvement_supported else \"0 is included ‚Üí MAPE difference is not statistically supported\"}**\n",
    "\n",
    "This addresses the key concern that a single test score could be unusually high (or low) by chance.\n",
    "\n",
    "## 4) Why **log-space R¬≤ (R¬≤_log)** is the primary metric (and why original-scale R¬≤ can be negative)\n",
    "\n",
    "NYC trip duration is **heavy-tailed and right-skewed**: most rides are short, but a small fraction are extremely long due to traffic, detours, incidents, or recording noise.  \n",
    "\n",
    "- On the **original scale (seconds)**, squared-error-based metrics (like RMSE and R¬≤) can be dominated by a few extreme trips.  \n",
    "- That dominance can make **original-scale R¬≤ negative**, even when the model is meaningfully accurate on the vast majority of typical trips.\n",
    "\n",
    "By evaluating in **log space** using `log1p(trip_duration)`:\n",
    "\n",
    "- We **compress the long tail**, so the model is judged on overall behavior rather than a few extreme outliers.\n",
    "- Errors are treated more **proportionally** (a 2-minute miss on a 10-minute trip matters more than a 2-minute miss on a 2-hour trip).\n",
    "- The resulting **R¬≤_log is more stable and more interpretable** for this skewed regression target.\n",
    "\n",
    "For transparency (and to support later cross-dataset comparison rules), we still report original-scale metrics and **MAPE**, but **R¬≤_log** is the most faithful ‚Äúprimary‚Äù indicator of general predictive performance for this dataset.\n",
    "\n",
    "## 5) Interpretation: what the model learned (feature importance)\n",
    "\n",
    "Permutation importance on the **validation set** shows that spatial geometry dominates:\n",
    "\n",
    "- **Top driver (both models):** `haversine_km`  \n",
    "- The NN also benefits from temporal encodings (hour/day cycles) and contextual proxies (e.g., vendor), suggesting it captures **nonlinear interactions** beyond a linear ridge model.\n",
    "- In the grouped view, the NN‚Äôs strongest feature group is: **{top_group_line}**\n",
    "\n",
    "## 6) Reproducibility + compute cost artifacts (submission readiness)\n",
    "\n",
    "- **Compute budget used:** {TRIALS} NN trials, total tuning time ‚âà **{total_time/60:.2f} minutes** on **{device}**.\n",
    "- **Environment snapshot written:** `../environment_summary.md` and `../pip_freeze.txt`.\n",
    "- **Model saved and reloadable:** `../artifacts/best_mlp_model.pt` + reload demo cell.\n",
    "\n",
    "## 7) Final takeaway (Phase 1)\n",
    "\n",
    "A carefully tuned MLP on engineered tabular features produces a **measurable and (via bootstrap) test-validated** improvement over a tuned linear baseline, while log-space evaluation provides a fair and stable view of performance under heavy-tailed trip durations. Phase 2 will expand the comparison by adding **EA optimization cost** and a more **interpretable fuzzy-system alternative**, enabling a principled trade-off analysis across accuracy, compute, and interpretability.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f03e01-6150-445d-a41e-a99c53677f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
